---
layout: post
title: 结合librec源代码读论文:因子机
author: Danyang Liu
---

本文结合librec源代码，解析因子机的原理。

### FMSGDRecommender

参考论文:[Rendle S. Factorization machines with libfm//TITS 2012](https://dl.acm.org/citation.cfm?id=2168771)

#### 模型

隐因子分解机（Factorization Machine，FM）旨在解决稀疏矩阵下的特征组合问题。传统机器学习问题，一般仅考虑如何对特征赋予权重，而没有考虑特征间存在相互作用，FM模型的提出较好地解决了该问题。

传统线性回归模型基于以下模型：

![一阶回归](../images/alg/fm/fm1.png)

传统线性回归模型没有 `\( x_i x_j \)` 的特征交叉，这样的模型不能体现特征之间的相互关系，并且由于特征空间巨大，数据稀疏性非常严重。

于是二阶多项式回归模型：

![二阶回归](../images/alg/fm/fm2.png)

这个模型虽然有个变量之间的相互关系，但是实际操作起来有很大的问题。`\( x_i x_j \)` 表示 `\( x_i \)` 和 `\( x_j \)` 的特征组合，只有当 `\( x_i \)` 和 `\( x_j \)` 都不为0时才有意义。
由于数据稀疏性，样本中出现未交互的特征分量是普遍存在的。

为了克服上面的问题，FM的作者，借鉴了之前矩阵分解的思想，把 `\( W \)` 矩阵用 `\( V V^T \)` 表示。 `\( w_{ij} = <v_i,v_j> \)` 其中：

![vivj表达式](../images/alg/fm/fm3.png)

这是FM的核心思想。

用矩阵分解学习`\( V \)`间接学习`\( W \)`有效的降低了学习参数个数，缓解了数据稀疏问题。

通过对公式的改写和利用`\( x \)`中绝大部分数据为0，可以将复杂度降为线性复杂度。

#### 学习算法

FM有三种主要的学习算法，随机梯度下降，交替方向乘子(ALS),基于蒙特卡罗马尔科夫链的优化方案。

1. 随机梯度下降，这种方案收敛慢而且非常敏感，可以利用现代的一些trick，例如采用 AdaGrad 算法，采用自适应学习率，效果相对比较好，FFM就采用这种方案。
2. 交替方向乘子(ALS)，这种方案只适用于回归问题，它每次优化一个参数，把其他参数固定，好处是每次都是一个最小二乘问题，有解析解。
3. 基于蒙特卡罗马尔科夫链的优化方案，论文中效果最好的方案。


### FMALSRecommender

参考论文:[Rendle S. Factorization machines with libfm//TITS 2012](https://dl.acm.org/citation.cfm?id=2168771)

#### 学习算法

本文参考：[Factorization Machines 学习笔记](http://blog.csdn.net/itplus/article/details/40534885)
        [因子机深入解析](http://tracholar.github.io/machine-learning/2017/03/10/factorization-machine.html)